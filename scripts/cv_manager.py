import os
import pickle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from typing import Dict, Any, Optional, Tuple
from argparse import Namespace
import optuna

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config import N_SPLITS, CV_SPLITS_DIR
from data_loaders import create_standardized_datasets
from train_test import train, test

class CrossValidationManager:
    """
    ç»Ÿä¸€çš„äº¤å‰éªŒè¯ç®¡ç†å™¨ã€‚
    è´Ÿè´£ç®¡ç† K-æŠ˜äº¤å‰éªŒè¯çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹ï¼Œå¹¶æ”¯æŒ Optuna å‰ªæã€‚
    """
    
    def __init__(self, opt: Namespace, device: torch.device):
        self.opt = opt
        self.device = device
        self.n_splits = N_SPLITS
        
        # ä½¿ç”¨æ¸…æ™°çš„å˜é‡å 'val' å’Œ 'train'
        self.results = {
            'train_cindices': [],  # ã€æ–°å¢ã€‘
            'val_cindices': [],
            'val_pvalues': [],
            'train_iaucs': [],     # ã€æ–°å¢I-AUCã€‘
            'val_iaucs': [],       # ã€æ–°å¢I-AUCã€‘
            'train_ibriers': [],   # ã€æ–°å¢I-Brierã€‘
            'val_ibriers': [],     # ã€æ–°å¢I-Brierã€‘
            'fold_results': []
        }
    
    def _get_model_paths(self, fold: int) -> Tuple[str, str]:
        # ... (ä»£ç ä¸å˜) ...
        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.exp_name, self.opt.model_name)
        weights_path = os.path.join(expr_dir, f'split_{fold}_best_weights.pt')
        metadata_path = os.path.join(expr_dir, f'split_{fold}_metadata.pkl')
        return weights_path, metadata_path
    
    def load_fold_data(self, fold: int) -> Dict[str, Any]:
        # ... (ä»£ç ä¸å˜) ...
        split_data_path = os.path.join(CV_SPLITS_DIR, f'split_{fold}_data.pkl')
        try:
            with open(split_data_path, 'rb') as f:
                data = pickle.load(f)
            return data
        except FileNotFoundError:
            raise FileNotFoundError(f"Data file not found: {split_data_path}. Please run the data preparation script first.")
    
    def save_model_and_metadata(self, model: nn.Module, fold: int, metric_logger: Dict) -> None:
        # ... (ä»£ç ä¸å˜) ...
        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.exp_name, self.opt.model_name)
        os.makedirs(expr_dir, exist_ok=True)
        
        weights_path, metadata_path = self._get_model_paths(fold)
        torch.save(model.state_dict(), weights_path)
        
        metadata = {'split': fold, 'opt': vars(self.opt), 'metrics': metric_logger}
        with open(metadata_path, 'wb') as f:
            pickle.dump(metadata, f)

    def run_training_cv(self, trial: Optional[optuna.trial.Trial] = None) -> Dict[str, Any]:
        """
        è¿è¡ŒK-æŠ˜äº¤å‰éªŒè¯çš„è®­ç»ƒæµç¨‹ï¼Œå¹¶è¯„ä¼°è®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚
        """
        print(f"ğŸš€ Starting {self.n_splits}-fold cross-validation training for model: {self.opt.model_name}...")
        
        for k in range(self.n_splits):
            print(f"\nğŸ“Š [Fold {k + 1}/{self.n_splits}]")
            
            fold_data_dict = self.load_fold_data(k)
            print("  - Data for fold loaded successfully.")
            
            model, _, metric_logger = train(self.opt, fold_data_dict, self.device, k)
            print("  - Model training completed.")
            
            # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼° ---
            train_dataset, val_dataset, _ = create_standardized_datasets(self.opt, fold_data_dict)
            
            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            _, cindex_val, pvalue_val, _, iauc_val, ibrier_val, timepoint_aucs_val, _ = test(self.opt, model, val_dataset, self.device)
            print(f"  - Final evaluation on validation set: C-Index = {cindex_val:.4f}, I-AUC = {iauc_val:.4f}, I-Brier = {ibrier_val:.4f}")

            # åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°
            _, cindex_train, _, _, iauc_train, ibrier_train, timepoint_aucs_train, _ = test(self.opt, model, train_dataset, self.device)
            print(f"  - Final evaluation on training set:   C-Index = {cindex_train:.4f}, I-AUC = {iauc_train:.4f}, I-Brier = {ibrier_train:.4f}")
            
            # è®°å½•ç»“æœ
            self.results['train_cindices'].append(cindex_train) # ã€æ–°å¢ã€‘
            self.results['val_cindices'].append(cindex_val)
            self.results['val_pvalues'].append(pvalue_val)
            self.results['train_iaucs'].append(iauc_train)      # ã€æ–°å¢I-AUCã€‘
            self.results['val_iaucs'].append(iauc_val)          # ã€æ–°å¢I-AUCã€‘
            self.results['train_ibriers'].append(ibrier_train)  # ã€æ–°å¢I-Brierã€‘
            self.results['val_ibriers'].append(ibrier_val)      # ã€æ–°å¢I-Brierã€‘
            
            fold_result = {
                'fold': k + 1, 
                'train_cindex': cindex_train, # ã€æ–°å¢ã€‘
                'val_cindex': cindex_val, 
                'val_pvalue': pvalue_val,
                'train_iauc': iauc_train,     # ã€æ–°å¢I-AUCã€‘
                'val_iauc': iauc_val,         # ã€æ–°å¢I-AUCã€‘
                'train_ibrier': ibrier_train, # ã€æ–°å¢I-Brierã€‘
                'val_ibrier': ibrier_val      # ã€æ–°å¢I-Brierã€‘
            }
            self.results['fold_results'].append(fold_result)
            self.save_model_and_metadata(model, k, metric_logger)
            print(f"  - Model weights and metadata for fold {k+1} have been saved.")

            if trial:
                trial.report(cindex_val, k)
                if trial.should_prune():
                    print(f"  - âœ‚ï¸ Trial pruned at fold {k+1} due to poor performance.")
                    raise optuna.exceptions.TrialPruned()
        
        return self._compute_final_statistics()

    def _compute_final_statistics(self) -> Dict[str, Any]:
        """è®¡ç®—äº¤å‰éªŒè¯çš„æœ€ç»ˆç»Ÿè®¡ç»“æœï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰ã€‚"""
        train_cindices_np = np.array(self.results['train_cindices']) # ã€æ–°å¢ã€‘
        val_cindices_np = np.array(self.results['val_cindices'])
        train_iaucs_np = np.array(self.results['train_iaucs'])       # ã€æ–°å¢I-AUCã€‘
        val_iaucs_np = np.array(self.results['val_iaucs'])           # ã€æ–°å¢I-AUCã€‘
        train_ibriers_np = np.array(self.results['train_ibriers'])   # ã€æ–°å¢I-Brierã€‘
        val_ibriers_np = np.array(self.results['val_ibriers'])       # ã€æ–°å¢I-Brierã€‘
        
        stats = {
            'mean_train_cindex': np.mean(train_cindices_np), # ã€æ–°å¢ã€‘
            'std_train_cindex': np.std(train_cindices_np),   # ã€æ–°å¢ã€‘
            'mean_val_cindex': np.mean(val_cindices_np),
            'std_val_cindex': np.std(val_cindices_np),
            'mean_train_iauc': np.mean(train_iaucs_np),      # ã€æ–°å¢I-AUCã€‘
            'std_train_iauc': np.std(train_iaucs_np),        # ã€æ–°å¢I-AUCã€‘
            'mean_val_iauc': np.mean(val_iaucs_np),          # ã€æ–°å¢I-AUCã€‘
            'std_val_iauc': np.std(val_iaucs_np),            # ã€æ–°å¢I-AUCã€‘
            'mean_train_ibrier': np.mean(train_ibriers_np),  # ã€æ–°å¢I-Brierã€‘
            'std_train_ibrier': np.std(train_ibriers_np),    # ã€æ–°å¢I-Brierã€‘
            'mean_val_ibrier': np.mean(val_ibriers_np),      # ã€æ–°å¢I-Brierã€‘
            'std_val_ibrier': np.std(val_ibriers_np),        # ã€æ–°å¢I-Brierã€‘
            'fold_results': self.results['fold_results']
        }
        return stats
    
    # save_results å’Œ print_summary å‡½æ•°å¯ä»¥ä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äº fold_results å’Œ _compute_final_statisticsï¼Œ
    # è€Œè¿™äº›å·²ç»æ›´æ–°äº†ã€‚ä¸è¿‡ä¸ºäº†æ¸…æ™°ï¼Œæˆ‘ä»¬ä¹Ÿä¸€å¹¶æ›´æ–°å®ƒä»¬ã€‚
    
    def save_results(self, results: Dict[str, Any]) -> str:
        """ä¿å­˜æ¯æŠ˜ä¸æ±‡æ€»ç»Ÿè®¡ç»“æœåˆ°å®éªŒç›®å½•ï¼Œå¹¶è¿”å›CSVè·¯å¾„ã€‚"""
        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.exp_name, self.opt.model_name)
        os.makedirs(expr_dir, exist_ok=True)

        # 1) ä¿å­˜æŒ‰æŠ˜ç»“æœ
        df = pd.DataFrame(results.get('fold_results', []))
        cv_csv_path = os.path.join(expr_dir, 'cv_results.csv')
        df.to_csv(cv_csv_path, index=False)

        # 2) ä¿å­˜æ±‡æ€»ç»Ÿè®¡
        summary = {
            'mean_train_cindex': float(results.get('mean_train_cindex', float('nan'))),
            'std_train_cindex': float(results.get('std_train_cindex', float('nan'))),
            'mean_val_cindex': float(results.get('mean_val_cindex', float('nan'))),
            'std_val_cindex': float(results.get('std_val_cindex', float('nan'))),
            'mean_train_iauc': float(results.get('mean_train_iauc', float('nan'))),
            'std_train_iauc': float(results.get('std_train_iauc', float('nan'))),
            'mean_val_iauc': float(results.get('mean_val_iauc', float('nan'))),
            'std_val_iauc': float(results.get('std_val_iauc', float('nan'))),
            'mean_train_ibrier': float(results.get('mean_train_ibrier', float('nan'))),
            'std_train_ibrier': float(results.get('std_train_ibrier', float('nan'))),
            'mean_val_ibrier': float(results.get('mean_val_ibrier', float('nan'))),
            'std_val_ibrier': float(results.get('std_val_ibrier', float('nan'))),
            'n_splits': int(self.n_splits),
        }
        summary_json_path = os.path.join(expr_dir, 'cv_summary.json')
        try:
            import json
            with open(summary_json_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•ä¿å­˜äº¤å‰éªŒè¯æ±‡æ€»åˆ° {summary_json_path}: {e}")

        return cv_csv_path

    def print_summary(self, results: Dict[str, Any]):
        """åœ¨æ§åˆ¶å°æ‰“å°äº¤å‰éªŒè¯ç»“æœçš„æ‘˜è¦ã€‚"""
        print("\n" + "="*80)
        print(f"ğŸ‰ Cross-Validation Completed: {self.opt.model_name}")
        print("="*80)
        df = pd.DataFrame(results['fold_results'])
        print("Fold-wise Performance on Train/Validation Sets:")
        # to_string ä¼šè‡ªåŠ¨æ‰“å°æ‰€æœ‰åˆ—
        print(df.to_string(index=False, float_format="%.4f")) 
        print("-" * 80)
        print(f"ğŸ¯ Average Train C-Index:      {results['mean_train_cindex']:.4f} Â± {results['std_train_cindex']:.4f}")
        print(f"ğŸ¯ Average Validation C-Index: {results['mean_val_cindex']:.4f} Â± {results['std_val_cindex']:.4f}")
        print(f"ğŸ¯ Average Train I-AUC:        {results['mean_train_iauc']:.4f} Â± {results['std_train_iauc']:.4f}")
        print(f"ğŸ¯ Average Validation I-AUC:   {results['mean_val_iauc']:.4f} Â± {results['std_val_iauc']:.4f}")
        print("="*80)
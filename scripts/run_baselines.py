import os
import pickle
import pandas as pd
import time
import numpy as np
import torch
import random
import torch.backends.cudnn as cudnn
from argparse import ArgumentParser
from typing import Optional

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—å’Œé…ç½®
from options import get_base_parser, parse_gpuids, print_options, normalize_mode, to_simplified_mode
from config import BASELINE_EXPERIMENTS, OPTIMIZED_HYPERPARAMS, EXTERNAL_DATA_DIR, CV_SPLITS_DIR, N_SPLITS, RESULTS_DIR
from logger_manager import LoggerManager
from data_loaders import MriWsiDataset
from networks import define_net
from train_test import test
from cv_manager import CrossValidationManager

# =======================================================================================
# æ ¸å¿ƒäº¤å‰éªŒè¯å‡½æ•° (ä» core_runner.py æ•´åˆè€Œæ¥)
# =======================================================================================
def run_cv_experiment(opt, parser=None, trial=None):
    """
    è¿è¡Œä¸€æ¬¡å®Œæ•´çš„äº¤å‰éªŒè¯å®éªŒã€‚
    parser åªæ˜¯å¯é€‰çš„ï¼Œç”¨äºæ‰“å°é…ç½®ã€‚
    """
    if not trial:
        print("\n" + "="*80)
        print(f"ğŸš€ å¼€å§‹è¿è¡Œå®éªŒ: {opt.exp_name} | æ¨¡å‹: {opt.model_name}")
        print("="*80)
        if parser: # åªæœ‰å½“ parser è¢«ä¼ å…¥æ—¶æ‰æ‰“å°
            print_options(opt, parser)
    
    # --- ç¯å¢ƒè®¾ç½® ---
    torch.manual_seed(2019); random.seed(2019); np.random.seed(2019)
    if opt.gpu_ids and torch.cuda.is_available(): torch.cuda.manual_seed_all(2019)
    cudnn.deterministic = True
    device = torch.device(f'cuda:{opt.gpu_ids[0]}') if opt.gpu_ids and torch.cuda.is_available() else torch.device('cpu')
    
    # --- é’ˆå¯¹èåˆæ¨¡å‹çš„é»˜è®¤æ­£åˆ™èŒƒå›´ï¼šä¼˜å…ˆåªçº¦æŸèåˆå±‚/åˆ†ç±»å¤´ ---
    try:
        fusion_modes = ['simple_fusion', 'multiscale_fusion', 'coattn', 'bilinear_fusion']
        if getattr(opt, 'mode', None) in fusion_modes:
            current_reg = getattr(opt, 'reg_type', 'rad')
            if current_reg in ['rad', 'path']:
                opt.reg_type = 'fusion'
    except Exception:
        pass

    # --- è¿è¡Œäº¤å‰éªŒè¯ ---
    cv_manager = CrossValidationManager(opt, device)
    # ã€æ ¸å¿ƒä¿®æ­£ã€‘ç¡®ä¿ trial å‚æ•°è¢«æ­£ç¡®ä¼ é€’
    results = cv_manager.run_training_cv(trial=trial)
    
    # --- æ‰“å°å’Œä¿å­˜ç»“æœ (ä»…åœ¨éè°ƒä¼˜æ¨¡å¼ä¸‹) ---
    if not trial:
        cv_manager.print_summary(results)
        cv_manager.save_results(results)
    
    return results

# =======================================================================================
# è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜)
# =======================================================================================
def evaluate_folds_on_external(opt_exp, external_test_data_raw, logger):
    """åœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒå‡ºçš„æ‰€æœ‰æ¨¡å‹ã€‚
    
    æä¾›ä¸¤ç§è¯„ä¼°æ¨¡å¼ï¼š
    1. ä¼ ç»Ÿæ¨¡å¼ï¼šæ¯ä¸ªæŠ˜åˆ†åˆ«è¯„ä¼°ï¼Œç„¶åè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    2. èšåˆæ¨¡å¼ï¼šèšåˆæ‰€æœ‰æŠ˜çš„é¢„æµ‹ç»“æœï¼ŒåŸºäºå¹³å‡é¢„æµ‹è®¡ç®—å•ä¸€æŒ‡æ ‡å€¼
    """
    logger.info(f"--- Starting evaluation on external test set for model: {opt_exp.model_name} ---")
    
    # å­˜å‚¨æ¯ä¸ªæŠ˜çš„é¢„æµ‹ç»“æœç”¨äºèšåˆ
    all_fold_predictions = []
    all_fold_survtimes = []
    all_fold_censors = []
    all_fold_ids = []
    
    # å­˜å‚¨æ¯ä¸ªæŠ˜çš„å•ç‹¬è¯„ä¼°ç»“æœï¼ˆç”¨äºä¼ ç»Ÿæ¨¡å¼ï¼‰
    cindex_results = []
    iauc_results = []
    ibrier_results = []
    timepoint_auc_results = {}
    
    per_fold_rows = []
    for k in range(N_SPLITS):
        # æ‰¾åˆ°å¯¹åº”æŠ˜çš„æ•°æ®ä»¥è·å–scaler
        split_data_path = os.path.join(CV_SPLITS_DIR, f'split_{k}_data.pkl')
        try:
            with open(split_data_path, 'rb') as f:
                fold_data = pickle.load(f)
        except FileNotFoundError:
            logger.warning(f"Could not find {split_data_path}, skipping fold {k}.")
            continue
            
        # åˆ›å»ºç”¨äºè·å–scalerçš„è®­ç»ƒé›†
        train_dataset_for_scaler = MriWsiDataset(opt_exp, fold_data, split='train')
        scalers = train_dataset_for_scaler.get_scalers()
        
        # åº”ç”¨scaleråˆ°å¤–éƒ¨æµ‹è¯•é›†
        external_dataset = MriWsiDataset(opt_exp, {'test': external_test_data_raw}, 'test', scalers)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        model_weights_path = os.path.join(opt_exp.checkpoints_dir, opt_exp.exp_name, opt_exp.model_name, f'split_{k}_best_weights.pt')
        if not os.path.exists(model_weights_path):
            logger.warning(f"Could not find model weights {model_weights_path}, skipping fold {k}.")
            continue

        device = torch.device(f'cuda:{opt_exp.gpu_ids[0]}') if opt_exp.gpu_ids and torch.cuda.is_available() else torch.device('cpu')
        model = define_net(opt_exp, k)
        model.load_state_dict(torch.load(model_weights_path, map_location='cpu'))
        model.to(device)
        
        # åœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæµ‹è¯•
        _, cindex_test, _, _, iauc_test, ibrier_test, timepoint_aucs_test, raw_results = test(opt_exp, model, external_dataset, device)
        
        # å­˜å‚¨åŸå§‹é¢„æµ‹ç»“æœç”¨äºèšåˆ
        risk_pred, survtime, censor = raw_results
        all_fold_predictions.append(risk_pred)
        all_fold_survtimes.append(survtime)
        all_fold_censors.append(censor)
        # IDsï¼ˆå¦‚æœ external_test_data_raw æä¾›äº† 'ids' å­—æ®µï¼Œåˆ™ä½¿ç”¨ï¼Œå¦åˆ™å ä½ï¼‰
        if 'ids' in external_test_data_raw:
            all_fold_ids.append(external_test_data_raw['ids'])
        
        # å­˜å‚¨å•ç‹¬è¯„ä¼°ç»“æœ
        cindex_results.append(cindex_test)
        iauc_results.append(iauc_test)
        ibrier_results.append(ibrier_test)
        
        # æ”¶é›†å››ä¸ªä¸´åºŠæ—¶é—´ç‚¹çš„AUCç»“æœ
        if k == 0:  # åˆå§‹åŒ–æ—¶é—´ç‚¹AUCç»“æœå­—å…¸
            timepoint_auc_results = {timepoint: [] for timepoint in timepoint_aucs_test.keys()}
        for timepoint, auc_value in timepoint_aucs_test.items():
            timepoint_auc_results[timepoint].append(auc_value)
        
        logger.info(f"  - Fold {k+1} external test C-Index: {cindex_test:.4f}, I-AUC: {iauc_test:.4f}, I-Brier: {ibrier_test:.4f}")
        logger.info(f"    Time-dependent AUCs: 1-year: {timepoint_aucs_test['1-year']:.4f}, 2-year: {timepoint_aucs_test['2-year']:.4f}, 3-year: {timepoint_aucs_test['3-year']:.4f}, 5-year: {timepoint_aucs_test['5-year']:.4f}")

        per_fold_rows.append({
            'fold': k + 1,
            'test_cindex': float(cindex_test),
            'test_iauc': float(iauc_test),
            'test_ibrier': float(ibrier_test),
            'auc_1-year': float(timepoint_aucs_test.get('1-year', float('nan'))),
            'auc_2-year': float(timepoint_aucs_test.get('2-year', float('nan'))),
            'auc_3-year': float(timepoint_aucs_test.get('3-year', float('nan'))),
            'auc_5-year': float(timepoint_aucs_test.get('5-year', float('nan'))),
        })

    if not cindex_results:
        logger.warning("Failed to evaluate any folds on the external test set.")
        return float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), {}, {}
    
    # === ä¼ ç»Ÿæ¨¡å¼ï¼šè®¡ç®—æ¯ä¸ªæŠ˜ç»“æœçš„å‡å€¼å’Œæ ‡å‡†å·® ===
    mean_cindex = np.mean(cindex_results)
    std_cindex = np.std(cindex_results)
    mean_iauc = np.mean(iauc_results)
    std_iauc = np.std(iauc_results)
    mean_ibrier = np.mean(ibrier_results)
    std_ibrier = np.std(ibrier_results)
    
    # è®¡ç®—å››ä¸ªä¸´åºŠæ—¶é—´ç‚¹AUCçš„å‡å€¼å’Œæ ‡å‡†å·®
    timepoint_auc_stats = {}
    for timepoint in timepoint_auc_results.keys():
        timepoint_auc_stats[timepoint] = {
            'mean': np.mean(timepoint_auc_results[timepoint]),
            'std': np.std(timepoint_auc_results[timepoint])
        }
    
    # === èšåˆæ¨¡å¼ï¼šåŸºäºå¹³å‡é¢„æµ‹è®¡ç®—å•ä¸€æŒ‡æ ‡å€¼ ===
    from utils import CIndex_lifeline, integrated_brier_score, integrated_auc, clinical_timepoints_auc
    
    # è®¡ç®—å¹³å‡é¢„æµ‹åˆ†æ•°ï¼ˆäº”æŠ˜çš„å¹³å‡ï¼‰
    avg_predictions = np.mean(all_fold_predictions, axis=0)
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæŠ˜çš„ç”Ÿå­˜æ—¶é—´å’Œåˆ å¤±çŠ¶æ€ï¼ˆæ‰€æœ‰æŠ˜åº”è¯¥ç›¸åŒï¼‰
    final_survtime = all_fold_survtimes[0]
    final_censor = all_fold_censors[0]
    
    # åŸºäºå¹³å‡é¢„æµ‹è®¡ç®—èšåˆæŒ‡æ ‡
    aggregated_cindex = CIndex_lifeline(avg_predictions, final_censor, final_survtime)
    aggregated_iauc = integrated_auc(avg_predictions, final_censor, final_survtime)
    aggregated_ibrier = integrated_brier_score(avg_predictions, final_censor, final_survtime)
    aggregated_timepoint_aucs = clinical_timepoints_auc(avg_predictions, final_censor, final_survtime)
    
    # è·å–æ‚£è€…IDä¿¡æ¯ï¼šä¼˜å…ˆä½¿ç”¨å¤–éƒ¨æµ‹è¯•æ•°æ®ä¸­çš„ ids å­—æ®µ
    if 'ids' in external_test_data_raw:
        patient_ids = external_test_data_raw['ids']
    else:
        patient_ids = [f'Patient_{i}' for i in range(len(avg_predictions))]
    
    # ä¿å­˜èšåˆé¢„æµ‹åˆ†æ•°ç”¨äºåç»­åˆ†æ
    aggregated_predictions_data = {
        'patient_ids': patient_ids,
        'predictions': avg_predictions,
        'survtime': final_survtime,
        'censor': final_censor,
        'model_name': opt_exp.model_name,
        'exp_name': opt_exp.exp_name
    }
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    predictions_save_dir = os.path.join(opt_exp.checkpoints_dir, opt_exp.exp_name, 'aggregated_predictions')
    os.makedirs(predictions_save_dir, exist_ok=True)
    
    # ä¿å­˜ä¸ºpickleæ ¼å¼ï¼ˆç”¨äºç¨‹åºåŠ è½½ï¼‰
    predictions_save_path_pkl = os.path.join(predictions_save_dir, f'{opt_exp.model_name}_aggregated_predictions.pkl')
    with open(predictions_save_path_pkl, 'wb') as f:
        pickle.dump(aggregated_predictions_data, f)
    
    # ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆç”¨æˆ·å‹å¥½æ ¼å¼ï¼‰
    predictions_save_path_csv = os.path.join(predictions_save_dir, f'{opt_exp.model_name}_aggregated_predictions.csv')
    predictions_df = pd.DataFrame({
        'case_id': patient_ids,
        'aggregated_prediction': avg_predictions,
        'survival_time': final_survtime,
        'censor_status': final_censor,
        'model_name': opt_exp.model_name,
        'experiment_name': opt_exp.exp_name
    })
    predictions_df.to_csv(predictions_save_path_csv, index=False, float_format='%.6f')
    
    logger.info(f"Aggregated predictions saved to:")
    logger.info(f"  - Pickle format: {predictions_save_path_pkl}")
    logger.info(f"  - CSV format: {predictions_save_path_csv}")
    
    # å¦å­˜æ¯æŠ˜å¤–éƒ¨æµ‹è¯•ç»“æœåˆ°æ¨¡å‹ç›®å½•
    try:
        import pandas as _pd
        model_dir = os.path.join(opt_exp.checkpoints_dir, opt_exp.exp_name, opt_exp.model_name)
        os.makedirs(model_dir, exist_ok=True)
        _pd.DataFrame(per_fold_rows).to_csv(os.path.join(model_dir, 'external_results.csv'), index=False, float_format='%.6f')
    except Exception as e:
        logger.warning(f"Failed to save per-fold external results CSV: {e}")

    # åˆ›å»ºèšåˆç»“æœå­—å…¸
    aggregated_results = {
        'cindex': aggregated_cindex,
        'iauc': aggregated_iauc,
        'ibrier': aggregated_ibrier,
        'timepoint_aucs': aggregated_timepoint_aucs,
        'predictions_path_pkl': predictions_save_path_pkl,  # pickleæ–‡ä»¶è·¯å¾„
        'predictions_path_csv': predictions_save_path_csv   # CSVæ–‡ä»¶è·¯å¾„
    }
    
    logger.info(f"--- Traditional mode (fold-wise average): C-Index {mean_cindex:.4f} Â± {std_cindex:.4f}, I-AUC {mean_iauc:.4f} Â± {std_iauc:.4f}, I-Brier {mean_ibrier:.4f} Â± {std_ibrier:.4f} ---")
    logger.info(f"--- Aggregated mode (ensemble prediction): C-Index {aggregated_cindex:.4f}, I-AUC {aggregated_iauc:.4f}, I-Brier {aggregated_ibrier:.4f} ---")
    logger.info(f"--- Traditional time-dependent AUCs:")
    for timepoint, stats in timepoint_auc_stats.items():
        logger.info(f"    {timepoint}: {stats['mean']:.4f} Â± {stats['std']:.4f}")
    logger.info(f"--- Aggregated time-dependent AUCs:")
    for timepoint, auc_value in aggregated_timepoint_aucs.items():
        logger.info(f"    {timepoint}: {auc_value:.4f}")
    logger.info("---")
    
    return mean_cindex, std_cindex, mean_iauc, std_iauc, mean_ibrier, std_ibrier, timepoint_auc_stats, aggregated_results


# =======================================================================================
# æ–°å¢ï¼šåèåˆï¼ˆlate_fusionï¼‰è¯„ä¼°å·¥å…·
# =======================================================================================
def _build_opt_for_model(args, model_mode, base_parser):
    """æ ¹æ® args æ„å»ºç”¨äºåŠ è½½æŒ‡å®šæ¨¡å‹çš„ optï¼ˆä¸ä¸€å®šè®­ç»ƒï¼‰ã€‚"""
    from options import parse_gpuids, normalize_mode, to_simplified_mode
    from config import OPTIMIZED_HYPERPARAMS
    opt = base_parser.parse_args([])
    opt.exp_name = args.exp_name if args.exp_name != 'fusion_experiment' else ('optimized_evaluation' if args.use_optimized else 'baseline_evaluation')
    internal_mode = normalize_mode(model_mode)
    opt.mode = internal_mode
    # ç»§æ‰¿ä¸ä¸»è¿è¡Œä¸€è‡´çš„åŸºç¡€å‚æ•°
    for k, v in vars(args).items():
        setattr(opt, k, v)
    # è¦†ç›–ä¼˜åŒ–è¶…å‚ï¼ˆè‹¥é€‰æ‹©ä½¿ç”¨ï¼‰
    if getattr(args, 'use_optimized', False) and internal_mode in OPTIMIZED_HYPERPARAMS:
        for k, v in OPTIMIZED_HYPERPARAMS[internal_mode].items():
            setattr(opt, k, v)
    # ä¸€è‡´çš„æ¨¡å‹å‘½åç­–ç•¥ï¼ˆä¸è¿½åŠ  _optimized åç¼€ï¼Œç»Ÿä¸€ç›®å½•å‘½åï¼‰
    opt.model_name = to_simplified_mode(internal_mode)
    opt = parse_gpuids(opt)
    return opt


def _weights_path_for(opt, fold_idx):
    import os
    return os.path.join(opt.checkpoints_dir, opt.exp_name, opt.model_name, f'split_{fold_idx}_best_weights.pt')


def _ensure_unimodal_models_trained(args, base_parser, logger):
    """è‹¥ç¼ºå°‘åèåˆæ‰€éœ€çš„å•æ¨¡æ€æƒé‡ï¼Œåˆ™è‡ªåŠ¨è®­ç»ƒå¯¹åº”æ¨¡å‹ã€‚"""
    from os.path import exists
    # æ„å»ºä¸¤ä¸ªå•æ¨¡æ€é…ç½®
    opt_rad = _build_opt_for_model(args, 'rad_only', base_parser)
    opt_path = _build_opt_for_model(args, 'path_only', base_parser)
    need_train_rad = not exists(_weights_path_for(opt_rad, 0))
    need_train_path = not exists(_weights_path_for(opt_path, 0))
    if not (need_train_rad or need_train_path):
        return opt_rad, opt_path
    logger.info("Late fusion requires unimodal weights. Missing detected -> training needed.")
    # è®­ç»ƒç¼ºå¤±è€…
    if need_train_rad:
        logger.info("Training missing unimodal model: rad_only ...")
        run_cv_experiment(opt=opt_rad, parser=base_parser, trial=None)
    if need_train_path:
        logger.info("Training missing unimodal model: path_only ...")
        run_cv_experiment(opt=opt_path, parser=base_parser, trial=None)
    return opt_rad, opt_path


def evaluate_late_fusion_on_cv(args, base_parser, logger):
    """åœ¨KæŠ˜CVä¸Šè¯„ä¼°åèåˆï¼ˆrad_only + path_only é£é™©å¹³å‡ï¼‰ã€‚"""
    import os
    import numpy as np
    import torch
    from config import CV_SPLITS_DIR, N_SPLITS
    from data_loaders import MriWsiDataset
    from networks import define_net
    from utils import CIndex_lifeline, integrated_auc, integrated_brier_score

    # ç¡®ä¿å•æ¨¡æ€æƒé‡å¯ç”¨
    opt_rad, opt_path = _ensure_unimodal_models_trained(args, base_parser, logger)

    device = torch.device(f'cuda:{opt_rad.gpu_ids[0]}') if opt_rad.gpu_ids and torch.cuda.is_available() else torch.device('cpu')

    train_cis, val_cis = [], []
    train_iaucs, val_iaucs = [], []
    train_ibriers, val_ibriers = [], []
    fold_results = []

    for k in range(N_SPLITS):
        # åŠ è½½å½“å‰æŠ˜æ•°æ®å¹¶æ„å»ºscaler
        import pickle
        split_data_path = os.path.join(CV_SPLITS_DIR, f'split_{k}_data.pkl')
        try:
            with open(split_data_path, 'rb') as f:
                fold_data = pickle.load(f)
        except FileNotFoundError:
            logger.warning(f"Could not find {split_data_path}, skipping fold {k}.")
            continue

        train_dataset_for_scaler = MriWsiDataset(opt_rad, fold_data, split='train')
        scalers = train_dataset_for_scaler.get_scalers()
        val_dataset = MriWsiDataset(opt_rad, fold_data, split='test', scalers=scalers)
        tr_dataset = MriWsiDataset(opt_rad, fold_data, split='train', scalers=scalers)

        # åŠ è½½ä¸¤ä¸ªå•æ¨¡æ€æ¨¡å‹
        m_rad = define_net(opt_rad, k)
        m_path = define_net(opt_path, k)
        rad_w = _weights_path_for(opt_rad, k)
        path_w = _weights_path_for(opt_path, k)
        if not (os.path.exists(rad_w) and os.path.exists(path_w)):
            logger.warning(f"Missing weights for fold {k}: rad_only -> {rad_w}, path_only -> {path_w}. Skipping fold.")
            continue
        m_rad.load_state_dict(torch.load(rad_w, map_location='cpu'))
        m_path.load_state_dict(torch.load(path_w, map_location='cpu'))
        m_rad.to(device); m_path.to(device)

        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å¹¶èåˆ
        _, _, _, _, _, _, _, raw_rad_val = test(opt_rad, m_rad, val_dataset, device)
        _, _, _, _, _, _, _, raw_path_val = test(opt_path, m_path, val_dataset, device)
        pred_val = (np.array(raw_rad_val[0]) + np.array(raw_path_val[0])) / 2.0
        surv_val = np.array(raw_rad_val[1]); cens_val = np.array(raw_rad_val[2])
        ci_val = CIndex_lifeline(pred_val, cens_val, surv_val)
        iauc_val = integrated_auc(pred_val, cens_val, surv_val)
        ibrier_val = integrated_brier_score(pred_val, cens_val, surv_val)

        # åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°å¹¶èåˆ
        _, _, _, _, _, _, _, raw_rad_tr = test(opt_rad, m_rad, tr_dataset, device)
        _, _, _, _, _, _, _, raw_path_tr = test(opt_path, m_path, tr_dataset, device)
        pred_tr = (np.array(raw_rad_tr[0]) + np.array(raw_path_tr[0])) / 2.0
        surv_tr = np.array(raw_rad_tr[1]); cens_tr = np.array(raw_rad_tr[2])
        ci_tr = CIndex_lifeline(pred_tr, cens_tr, surv_tr)
        iauc_tr = integrated_auc(pred_tr, cens_tr, surv_tr)
        ibrier_tr = integrated_brier_score(pred_tr, cens_tr, surv_tr)

        train_cis.append(ci_tr); val_cis.append(ci_val)
        train_iaucs.append(iauc_tr); val_iaucs.append(iauc_val)
        train_ibriers.append(ibrier_tr); val_ibriers.append(ibrier_val)
        fold_results.append({'fold': k + 1, 'train_cindex': ci_tr, 'val_cindex': ci_val, 'train_iauc': iauc_tr, 'val_iauc': iauc_val, 'train_ibrier': ibrier_tr, 'val_ibrier': ibrier_val})

        logger.info(f"  - [LateFusion] Fold {k+1}: Train CI={ci_tr:.4f}, Val CI={ci_val:.4f}, Val I-AUC={iauc_val:.4f}, Val I-Brier={ibrier_val:.4f}")

    if not val_cis:
        logger.warning("Late fusion CV evaluation failed: no valid folds.")
        return {}

    results = {
        'mean_train_cindex': float(np.mean(train_cis)),
        'std_train_cindex': float(np.std(train_cis)),
        'mean_val_cindex': float(np.mean(val_cis)),
        'std_val_cindex': float(np.std(val_cis)),
        'mean_train_iauc': float(np.mean(train_iaucs)),
        'std_train_iauc': float(np.std(train_iaucs)),
        'mean_val_iauc': float(np.mean(val_iaucs)),
        'std_val_iauc': float(np.std(val_iaucs)),
        'mean_train_ibrier': float(np.mean(train_ibriers)),
        'std_train_ibrier': float(np.std(train_ibriers)),
        'mean_val_ibrier': float(np.mean(val_ibriers)),
        'std_val_ibrier': float(np.std(val_ibriers)),
        'fold_results': fold_results,
    }
    # ä¿å­˜æ¯æŠ˜ CV ç»“æœï¼ˆè®­ç»ƒ/éªŒè¯ï¼‰
    try:
        import pandas as _pd
        model_dir = os.path.join(opt_rad.checkpoints_dir, opt_rad.exp_name, 'LateFusionNet')
        os.makedirs(model_dir, exist_ok=True)
        _pd.DataFrame(fold_results).to_csv(os.path.join(model_dir, 'cv_results.csv'), index=False, float_format='%.6f')
    except Exception as e:
        logger.warning(f"Failed to save LateFusionNet CV results CSV: {e}")
    return results


def evaluate_late_fusion_on_external(args, external_test_data_raw, base_parser, logger):
    """åœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°åèåˆï¼ˆrad_only + path_only é£é™©å¹³å‡ï¼‰ã€‚"""
    import os
    import numpy as np
    import torch
    from config import CV_SPLITS_DIR, N_SPLITS
    from data_loaders import MriWsiDataset
    from networks import define_net
    from utils import CIndex_lifeline, integrated_auc, integrated_brier_score, clinical_timepoints_auc
    import pickle

    # ç¡®ä¿å•æ¨¡æ€æƒé‡å¯ç”¨
    opt_rad, opt_path = _ensure_unimodal_models_trained(args, base_parser, logger)

    device = torch.device(f'cuda:{opt_rad.gpu_ids[0]}') if opt_rad.gpu_ids and torch.cuda.is_available() else torch.device('cpu')

    all_fold_predictions = []
    all_fold_survtimes = []
    all_fold_censors = []

    cindex_results = []
    iauc_results = []
    ibrier_results = []
    timepoint_auc_results = {}

    per_fold_rows = []
    for k in range(N_SPLITS):
        split_data_path = os.path.join(CV_SPLITS_DIR, f'split_{k}_data.pkl')
        try:
            with open(split_data_path, 'rb') as f:
                fold_data = pickle.load(f)
        except FileNotFoundError:
            logger.warning(f"Could not find {split_data_path}, skipping fold {k}.")
            continue

        # æ‹¿åˆ°è®­ç»ƒscalerå¹¶åº”ç”¨åˆ°å¤–éƒ¨é›†
        train_dataset_for_scaler = MriWsiDataset(opt_rad, fold_data, split='train')
        scalers = train_dataset_for_scaler.get_scalers()
        external_dataset = MriWsiDataset(opt_rad, {'test': external_test_data_raw}, 'test', scalers)

        # åŠ è½½ä¸¤ä¸ªæ¨¡å‹
        m_rad = define_net(opt_rad, k)
        m_path = define_net(opt_path, k)
        rad_w = _weights_path_for(opt_rad, k)
        path_w = _weights_path_for(opt_path, k)
        if not (os.path.exists(rad_w) and os.path.exists(path_w)):
            logger.warning(f"Missing weights for fold {k}: rad_only -> {rad_w}, path_only -> {path_w}. Skipping fold.")
            continue
        m_rad.load_state_dict(torch.load(rad_w, map_location='cpu'))
        m_path.load_state_dict(torch.load(path_w, map_location='cpu'))
        m_rad.to(device); m_path.to(device)

        # åˆ†åˆ«é¢„æµ‹å¹¶åèåˆ
        _, _, _, _, _, _, timepoint_aucs_rad, raw_rad = test(opt_rad, m_rad, external_dataset, device)
        _, _, _, _, _, _, timepoint_aucs_path, raw_path = test(opt_path, m_path, external_dataset, device)
        pred = (np.array(raw_rad[0]) + np.array(raw_path[0])) / 2.0
        surv = np.array(raw_rad[1]); cens = np.array(raw_rad[2])

        cindex_test = CIndex_lifeline(pred, cens, surv)
        iauc_test = integrated_auc(pred, cens, surv)
        ibrier_test = integrated_brier_score(pred, cens, surv)
        t_auc = clinical_timepoints_auc(pred, cens, surv)

        all_fold_predictions.append(pred)
        all_fold_survtimes.append(surv)
        all_fold_censors.append(cens)

        cindex_results.append(cindex_test)
        iauc_results.append(iauc_test)
        ibrier_results.append(ibrier_test)

        if not timepoint_auc_results:
            timepoint_auc_results = {tp: [] for tp in t_auc.keys()}
        for tp, auc_val in t_auc.items():
            timepoint_auc_results[tp].append(auc_val)

        logger.info(f"  - [LateFusion] Fold {k+1} external: C-Index={cindex_test:.4f}, I-AUC={iauc_test:.4f}, I-Brier={ibrier_test:.4f}")
        per_fold_rows.append({
            'fold': k + 1,
            'test_cindex': float(cindex_test),
            'test_iauc': float(iauc_test),
            'test_ibrier': float(ibrier_test),
            'auc_1-year': float(t_auc.get('1-year', float('nan'))),
            'auc_2-year': float(t_auc.get('2-year', float('nan'))),
            'auc_3-year': float(t_auc.get('3-year', float('nan'))),
            'auc_5-year': float(t_auc.get('5-year', float('nan'))),
        })

    if not cindex_results:
        logger.warning("No valid folds for late fusion external evaluation.")
        return float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), {}, {}

    mean_cindex = float(np.mean(cindex_results)); std_cindex = float(np.std(cindex_results))
    mean_iauc = float(np.mean(iauc_results)); std_iauc = float(np.std(iauc_results))
    mean_ibrier = float(np.mean(ibrier_results)); std_ibrier = float(np.std(ibrier_results))

    timepoint_auc_stats = {tp: {'mean': float(np.nanmean(vals)), 'std': float(np.nanstd(vals))} for tp, vals in timepoint_auc_results.items()}

    # èšåˆæ¨¡å¼
    from utils import CIndex_lifeline as _C, integrated_auc as _IA, integrated_brier_score as _IB, clinical_timepoints_auc as _CTA
    avg_predictions = np.mean(all_fold_predictions, axis=0)
    final_survtime = all_fold_survtimes[0]
    final_censor = all_fold_censors[0]
    aggregated_cindex = _C(avg_predictions, final_censor, final_survtime)
    aggregated_iauc = _IA(avg_predictions, final_censor, final_survtime)
    aggregated_ibrier = _IB(avg_predictions, final_censor, final_survtime)
    aggregated_timepoint_aucs = _CTA(avg_predictions, final_censor, final_survtime)

    predictions_save_dir = os.path.join(opt_rad.checkpoints_dir, opt_rad.exp_name, 'aggregated_predictions')
    os.makedirs(predictions_save_dir, exist_ok=True)
    import pandas as pd
    predictions_save_path_pkl = os.path.join(predictions_save_dir, 'late_fusion_aggregated_predictions.pkl')
    predictions_save_path_csv = os.path.join(predictions_save_dir, 'late_fusion_aggregated_predictions.csv')
    with open(predictions_save_path_pkl, 'wb') as f:
        import pickle as _p
        _p.dump({'predictions': avg_predictions, 'survtime': final_survtime, 'censor': final_censor, 'model_name': 'late_fusion', 'exp_name': opt_rad.exp_name}, f)
    pd.DataFrame({'aggregated_prediction': avg_predictions, 'survival_time': final_survtime, 'censor_status': final_censor, 'model_name': 'late_fusion', 'experiment_name': opt_rad.exp_name}).to_csv(predictions_save_path_csv, index=False, float_format='%.6f')

    # ä¿å­˜æ¯æŠ˜å¤–éƒ¨æµ‹è¯•ç»“æœï¼ˆLateFusionNetï¼‰
    try:
        import pandas as _pd
        model_dir = os.path.join(opt_rad.checkpoints_dir, opt_rad.exp_name, 'LateFusionNet')
        os.makedirs(model_dir, exist_ok=True)
        _pd.DataFrame(per_fold_rows).to_csv(os.path.join(model_dir, 'external_results.csv'), index=False, float_format='%.6f')
    except Exception as e:
        logger.warning(f"Failed to save LateFusionNet external results CSV: {e}")

    aggregated_results = {
        'cindex': aggregated_cindex,
        'iauc': aggregated_iauc,
        'ibrier': aggregated_ibrier,
        'timepoint_aucs': aggregated_timepoint_aucs,
        'predictions_path_pkl': predictions_save_path_pkl,
        'predictions_path_csv': predictions_save_path_csv,
    }

    return mean_cindex, std_cindex, mean_iauc, std_iauc, mean_ibrier, std_ibrier, timepoint_auc_stats, aggregated_results

def load_external_test_data(logger):
    """åŠ è½½å¤–éƒ¨æµ‹è¯•é›†æ•°æ®ã€‚"""
    external_test_path = os.path.join(EXTERNAL_DATA_DIR, 'external_test_data.pkl')
    try:
        with open(external_test_path, 'rb') as f:
            data = pickle.load(f)['test']
        logger.info(f"Successfully loaded external test set: {external_test_path}")
        return data
    except Exception as e:
        logger.error(f"Failed to load external test data: {e}. Please run 1_prepare_all_datasets.py first.", exc_info=True)
        return None

# =======================================================================================
# ä¸»å‡½æ•°
# =======================================================================================
def main():
    # 1. å‚æ•°è§£æå’Œæ—¥å¿—è®¾ç½®
    base_parser = get_base_parser()
    base_parser.add_argument('--use_optimized', action='store_true', default=True, help="Use optimized hyperparameters (default: True)")
    base_parser.add_argument('--use_baseline', action='store_true', help="Use baseline hyperparameters instead of optimized")
    all_model_modes = [exp['params']['mode'] for exp in BASELINE_EXPERIMENTS]
    simplified_defaults = [to_simplified_mode(m) for m in all_model_modes]
    extended_modes = simplified_defaults + ['LateFusionNet']
    base_parser.add_argument('--models_to_run', nargs='+', type=str, default=extended_modes, help="Specify models to run (support 'LateFusionNet')")
    base_parser.add_argument('--pretrain_exp_name', type=str, default='', help='Experiment name containing pretrained weights')
    args = base_parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†use_baselineï¼Œåˆ™ä¸ä½¿ç”¨ä¼˜åŒ–å‚æ•°
    if args.use_baseline:
        args.use_optimized = False
    
    EVALUATION_NAME = args.exp_name if args.exp_name != 'fusion_experiment' else ('optimized_evaluation' if args.use_optimized else 'baseline_evaluation')
    logger_manager = LoggerManager(experiment_name=EVALUATION_NAME)
    logger = logger_manager.get_logger(__name__)
    
    logger.info("="*80 + f"\nğŸš€ Starting Model Evaluation Workflow ({'Optimized Params' if args.use_optimized else 'Default Baseline Params'})")
    logger.info(f"Experiment Set Name: {EVALUATION_NAME}")
    logger.info(f"Models to run: {', '.join(args.models_to_run)}" + "\n" + "="*80)
    
    external_data_raw = load_external_test_data(logger)
    if external_data_raw is None: return

    # 2. é€‰æ‹©å‚æ•°æ¥æº (é»˜è®¤ä½¿ç”¨ä¼˜åŒ–å‚æ•°)
    if args.use_optimized:
        # keys ä¸ºå†…éƒ¨æ¨¡å¼å
        experiments_source = {model: {'name': f"{to_simplified_mode(model)}", 'params': params} for model, params in OPTIMIZED_HYPERPARAMS.items()}
    else:
        experiments_source = {exp['params']['mode']: exp for exp in BASELINE_EXPERIMENTS}

    all_final_results = []
    # 3. å¾ªç¯æ‰§è¡Œå®éªŒ
    for model_mode_in in args.models_to_run:
        mode_internal = normalize_mode(model_mode_in)
        mode_display = to_simplified_mode(mode_internal)
        if mode_internal == 'late_fusion':
            logger.info(f"\n{'='*80}\nâ¡ï¸  Running Experiment: LateFusionNet\n{'='*80}")
            # CVè¯„ä¼°ï¼ˆä¸è®­ç»ƒï¼‰
            cv_results = evaluate_late_fusion_on_cv(args, base_parser, logger)
            # å¤–éƒ¨è¯„ä¼°
            mean_ext_ci, std_ext_ci, mean_ext_iauc, std_ext_iauc, mean_ext_ibrier, std_ext_ibrier, timepoint_auc_stats, aggregated_results = evaluate_late_fusion_on_external(args, external_data_raw, base_parser, logger)
            result_dict = {
                'Model Architecture': 'LateFusionNet',
                'CV Train C-Index': cv_results.get('mean_train_cindex', float('nan')),
                'CV Val C-Index': cv_results.get('mean_val_cindex', float('nan')),
                'CV Val Std': cv_results.get('std_val_cindex', float('nan')),
                'CV Train I-AUC': cv_results.get('mean_train_iauc', float('nan')),
                'CV Val I-AUC': cv_results.get('mean_val_iauc', float('nan')),
                'CV Train I-Brier': cv_results.get('mean_train_ibrier', float('nan')),
                'CV Val I-Brier': cv_results.get('mean_val_ibrier', float('nan')),
                'External Test C-Index': mean_ext_ci,
                'External Test Std': std_ext_ci,
                'External Test I-AUC': mean_ext_iauc,
                'External Test I-AUC Std': std_ext_iauc,
                'External Test I-Brier': mean_ext_ibrier,
                'External Test I-Brier Std': std_ext_ibrier,
            }
            for timepoint, stats in timepoint_auc_stats.items():
                result_dict[f'External Test {timepoint} AUC'] = stats['mean']
                result_dict[f'External Test {timepoint} AUC Std'] = stats['std']
            result_dict['External Test C-Index (Aggregated)'] = aggregated_results['cindex']
            result_dict['External Test I-AUC (Aggregated)'] = aggregated_results['iauc']
            result_dict['External Test I-Brier (Aggregated)'] = aggregated_results['ibrier']
            for timepoint, auc_value in aggregated_results['timepoint_aucs'].items():
                result_dict[f'External Test {timepoint} AUC (Aggregated)'] = auc_value
            all_final_results.append(result_dict)
            continue

        # å¸¸è§„æ¨¡å‹è·¯å¾„
        if mode_internal not in experiments_source:
            logger.warning(f"Configuration for model '{model_mode_in}' not found in source, skipping.")
            continue
        exp_config = experiments_source[mode_internal]
        logger.info(f"\n{'='*80}\nâ¡ï¸  Running Experiment: {to_simplified_mode(mode_internal)}\n{'='*80}")

        opt = base_parser.parse_args([]) # Create a fresh opt object
        opt.exp_name = EVALUATION_NAME
        opt.mode = mode_internal
        for key, value in vars(args).items(): setattr(opt, key, value)
        for key, value in exp_config['params'].items(): setattr(opt, key, value)
        opt.model_name = mode_display
        opt = parse_gpuids(opt)

        cv_results = run_cv_experiment(opt=opt, parser=base_parser, trial=None)
        mean_ext_ci, std_ext_ci, mean_ext_iauc, std_ext_iauc, mean_ext_ibrier, std_ext_ibrier, timepoint_auc_stats, aggregated_results = evaluate_folds_on_external(opt, external_data_raw, logger)

        result_dict = {
            'Model Architecture': exp_config['name'], 
            'CV Train C-Index': cv_results.get('mean_train_cindex', float('nan')),
            'CV Val C-Index': cv_results.get('mean_val_cindex', float('nan')), 
            'CV Val Std': cv_results.get('std_val_cindex', float('nan')),
            'CV Train I-AUC': cv_results.get('mean_train_iauc', float('nan')),
            'CV Val I-AUC': cv_results.get('mean_val_iauc', float('nan')),
            'CV Train I-Brier': cv_results.get('mean_train_ibrier', float('nan')),
            'CV Val I-Brier': cv_results.get('mean_val_ibrier', float('nan')),
            'External Test C-Index': mean_ext_ci, 
            'External Test Std': std_ext_ci,
            'External Test I-AUC': mean_ext_iauc,
            'External Test I-AUC Std': std_ext_iauc,
            'External Test I-Brier': mean_ext_ibrier,
            'External Test I-Brier Std': std_ext_ibrier
        }
        for timepoint, stats in timepoint_auc_stats.items():
            result_dict[f'External Test {timepoint} AUC'] = stats['mean']
            result_dict[f'External Test {timepoint} AUC Std'] = stats['std']
        result_dict['External Test C-Index (Aggregated)'] = aggregated_results['cindex']
        result_dict['External Test I-AUC (Aggregated)'] = aggregated_results['iauc']
        result_dict['External Test I-Brier (Aggregated)'] = aggregated_results['ibrier']
        for timepoint, auc_value in aggregated_results['timepoint_aucs'].items():
            result_dict[f'External Test {timepoint} AUC (Aggregated)'] = auc_value
        all_final_results.append(result_dict)

    # 4. ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    if all_final_results:
        df = pd.DataFrame(all_final_results).sort_values(by='External Test C-Index', ascending=False).reset_index(drop=True)
        
        logger.info("\n\n" + "="*120)
        logger.info("--- Final Performance Summary: Cross-Validation vs. External Test Set ---")
        logger.info("="*120 + "\n")
        
        # å®šä¹‰è¦æ˜¾ç¤ºçš„åˆ—é¡ºåº
        display_cols = [
            'Model Architecture', 
            'CV Train C-Index', 
            'CV Val C-Index', 
            'CV Val Std', # ã€æ–°å¢ã€‘
            'CV Train I-AUC',  # ã€æ–°å¢I-AUCã€‘
            'CV Val I-AUC',    # ã€æ–°å¢I-AUCã€‘
            'CV Train I-Brier',  # ã€æ–°å¢I-Brierã€‘
            'CV Val I-Brier',    # ã€æ–°å¢I-Brierã€‘
            'External Test C-Index', 
            'External Test Std',
            'External Test C-Index (Aggregated)',  # ã€æ–°å¢èšåˆC-Indexã€‘
            'External Test I-AUC',      # ã€æ–°å¢å¤–éƒ¨æµ‹è¯•I-AUCã€‘
            'External Test I-AUC Std',   # ã€æ–°å¢å¤–éƒ¨æµ‹è¯•I-AUCæ ‡å‡†å·®ã€‘
            'External Test I-AUC (Aggregated)',    # ã€æ–°å¢èšåˆI-AUCã€‘
            'External Test I-Brier',      # ã€æ–°å¢å¤–éƒ¨æµ‹è¯•I-Brierã€‘
            'External Test I-Brier Std',   # ã€æ–°å¢å¤–éƒ¨æµ‹è¯•I-Brieræ ‡å‡†å·®ã€‘
            'External Test I-Brier (Aggregated)',  # ã€æ–°å¢èšåˆI-Brierã€‘
            'External Test 1-year AUC',    # ã€æ–°å¢1å¹´AUCã€‘
            'External Test 1-year AUC Std', # ã€æ–°å¢1å¹´AUCæ ‡å‡†å·®ã€‘
            'External Test 1-year AUC (Aggregated)', # ã€æ–°å¢èšåˆ1å¹´AUCã€‘
            'External Test 2-year AUC',    # ã€æ–°å¢2å¹´AUCã€‘
            'External Test 2-year AUC Std', # ã€æ–°å¢2å¹´AUCæ ‡å‡†å·®ã€‘
            'External Test 2-year AUC (Aggregated)', # ã€æ–°å¢èšåˆ2å¹´AUCã€‘
            'External Test 3-year AUC',    # ã€æ–°å¢3å¹´AUCã€‘
            'External Test 3-year AUC Std', # ã€æ–°å¢3å¹´AUCæ ‡å‡†å·®ã€‘
            'External Test 3-year AUC (Aggregated)', # ã€æ–°å¢èšåˆ3å¹´AUCã€‘
            'External Test 5-year AUC',    # ã€æ–°å¢5å¹´AUCã€‘
            'External Test 5-year AUC Std',  # ã€æ–°å¢5å¹´AUCæ ‡å‡†å·®ã€‘
            'External Test 5-year AUC (Aggregated)'  # ã€æ–°å¢èšåˆ5å¹´AUCã€‘
        ]
        
        # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„åˆ—éƒ½å­˜åœ¨ï¼Œä»¥é˜²ä¸‡ä¸€
        for col in display_cols:
            if col not in df.columns:
                df[col] = float('nan')
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        logger.info("\n" + df[display_cols].to_string(index=False, float_format="%.4f"))
        logger.info("\n" + "="*120)
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶
        report_path = os.path.join(RESULTS_DIR, f'{EVALUATION_NAME}_complete_report.csv')
        df.to_csv(report_path, index=False, float_format="%.4f")
        logger.info(f"\nComplete report saved to: {report_path}")

if __name__ == '__main__':
    main()
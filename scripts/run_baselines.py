import os
import pickle
import pandas as pd
import time
import numpy as np
import torch
import random
import torch.backends.cudnn as cudnn
from argparse import ArgumentParser
from typing import Optional

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—å’Œé…ç½®
from options import get_base_parser, parse_gpuids, print_options, normalize_mode, to_simplified_mode
from config import BASELINE_EXPERIMENTS, OPTIMIZED_HYPERPARAMS, EXTERNAL_DATA_DIR, CV_SPLITS_DIR, N_SPLITS, RESULTS_DIR
from logger_manager import LoggerManager
from data_loaders import MriWsiDataset
from networks import define_net
from train_test import test
from cv_manager import CrossValidationManager

# =======================================================================================
# æ ¸å¿ƒäº¤å‰éªŒè¯å‡½æ•° (ä» core_runner.py æ•´åˆè€Œæ¥)
# =======================================================================================
def run_cv_experiment(opt, parser=None, trial=None):
    """
    è¿è¡Œä¸€æ¬¡å®Œæ•´çš„äº¤å‰éªŒè¯å®éªŒã€‚
    parser åªæ˜¯å¯é€‰çš„ï¼Œç”¨äºæ‰“å°é…ç½®ã€‚
    """
    if not trial:
        print("\n" + "="*80)
        print(f"ğŸš€ å¼€å§‹è¿è¡Œå®éªŒ: {opt.exp_name} | æ¨¡å‹: {opt.model_name}")
        print("="*80)
        if parser: # åªæœ‰å½“ parser è¢«ä¼ å…¥æ—¶æ‰æ‰“å°
            print_options(opt, parser)
    
    # --- ç¯å¢ƒè®¾ç½® ---
    torch.manual_seed(2019); random.seed(2019); np.random.seed(2019)
    if opt.gpu_ids and torch.cuda.is_available(): torch.cuda.manual_seed_all(2019)
    cudnn.deterministic = True
    device = torch.device(f'cuda:{opt.gpu_ids[0]}') if opt.gpu_ids and torch.cuda.is_available() else torch.device('cpu')
    
    # --- é’ˆå¯¹èåˆæ¨¡å‹çš„é»˜è®¤æ­£åˆ™èŒƒå›´ï¼šä¼˜å…ˆåªçº¦æŸèåˆå±‚/åˆ†ç±»å¤´ ---
    try:
        fusion_modes = ['EarlyFusionNet', 'BilinearFusionNet', 'LateFusionNet']
        if getattr(opt, 'mode', None) in fusion_modes:
            current_reg = getattr(opt, 'reg_type', 'rad')
            if current_reg in ['rad', 'path']:
                opt.reg_type = 'fusion'
    except Exception:
        pass

    # --- è¿è¡Œäº¤å‰éªŒè¯ ---
    cv_manager = CrossValidationManager(opt, device)
    # ã€æ ¸å¿ƒä¿®æ­£ã€‘ç¡®ä¿ trial å‚æ•°è¢«æ­£ç¡®ä¼ é€’
    results = cv_manager.run_training_cv(trial=trial)
    
    # --- æ‰“å°å’Œä¿å­˜ç»“æœ (ä»…åœ¨éè°ƒä¼˜æ¨¡å¼ä¸‹) ---
    if not trial:
        cv_manager.print_summary(results)
        cv_manager.save_results(results)
    
    return results

# =======================================================================================
# è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜)
# =======================================================================================
def evaluate_folds_on_external(opt_exp, external_test_data_raw, logger):
    """åœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒå‡ºçš„æ‰€æœ‰æ¨¡å‹ã€‚
    
    æä¾›ä¸¤ç§è¯„ä¼°æ¨¡å¼ï¼š
    1. ä¼ ç»Ÿæ¨¡å¼ï¼šæ¯ä¸ªæŠ˜åˆ†åˆ«è¯„ä¼°ï¼Œç„¶åè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    2. èšåˆæ¨¡å¼ï¼šèšåˆæ‰€æœ‰æŠ˜çš„é¢„æµ‹ç»“æœï¼ŒåŸºäºå¹³å‡é¢„æµ‹è®¡ç®—å•ä¸€æŒ‡æ ‡å€¼
    """
    logger.info(f"--- Starting evaluation on external test set for model: {opt_exp.model_name} ---")
    
    # å­˜å‚¨æ¯ä¸ªæŠ˜çš„é¢„æµ‹ç»“æœç”¨äºèšåˆ
    all_fold_predictions = []
    all_fold_survtimes = []
    all_fold_censors = []
    all_fold_ids = []
    
    # å­˜å‚¨æ¯ä¸ªæŠ˜çš„å•ç‹¬è¯„ä¼°ç»“æœï¼ˆç”¨äºä¼ ç»Ÿæ¨¡å¼ï¼‰
    cindex_results = []
    iauc_results = []
    ibrier_results = []
    timepoint_auc_results = {}
    
    per_fold_rows = []
    for k in range(N_SPLITS):
        # æ‰¾åˆ°å¯¹åº”æŠ˜çš„æ•°æ®ä»¥è·å–scaler
        split_data_path = os.path.join(CV_SPLITS_DIR, f'split_{k}_data.pkl')
        try:
            with open(split_data_path, 'rb') as f:
                fold_data = pickle.load(f)
        except FileNotFoundError:
            logger.warning(f"Could not find {split_data_path}, skipping fold {k}.")
            continue
            
        # åˆ›å»ºç”¨äºè·å–scalerçš„è®­ç»ƒé›†
        train_dataset_for_scaler = MriWsiDataset(opt_exp, fold_data, split='train')
        scalers = train_dataset_for_scaler.get_scalers()
        
        # åº”ç”¨scaleråˆ°å¤–éƒ¨æµ‹è¯•é›†
        external_dataset = MriWsiDataset(opt_exp, {'test': external_test_data_raw}, 'test', scalers)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        model_weights_path = os.path.join(opt_exp.checkpoints_dir, opt_exp.exp_name, opt_exp.model_name, f'split_{k}_best_weights.pt')
        if not os.path.exists(model_weights_path):
            logger.warning(f"Could not find model weights {model_weights_path}, skipping fold {k}.")
            continue

        device = torch.device(f'cuda:{opt_exp.gpu_ids[0]}') if opt_exp.gpu_ids and torch.cuda.is_available() else torch.device('cpu')
        model = define_net(opt_exp, k)
        model.load_state_dict(torch.load(model_weights_path, map_location='cpu'))
        model.to(device)
        
        # åœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæµ‹è¯•
        _, cindex_test, _, _, iauc_test, ibrier_test, timepoint_aucs_test, raw_results = test(opt_exp, model, external_dataset, device)
        
        # å­˜å‚¨åŸå§‹é¢„æµ‹ç»“æœç”¨äºèšåˆ
        risk_pred, survtime, censor = raw_results
        all_fold_predictions.append(risk_pred)
        all_fold_survtimes.append(survtime)
        all_fold_censors.append(censor)
        # IDsï¼ˆå¦‚æœ external_test_data_raw æä¾›äº† 'ids' å­—æ®µï¼Œåˆ™ä½¿ç”¨ï¼Œå¦åˆ™å ä½ï¼‰
        if 'ids' in external_test_data_raw:
            all_fold_ids.append(external_test_data_raw['ids'])
        
        # å­˜å‚¨å•ç‹¬è¯„ä¼°ç»“æœ
        cindex_results.append(cindex_test)
        iauc_results.append(iauc_test)
        ibrier_results.append(ibrier_test)
        
        # æ”¶é›†å››ä¸ªä¸´åºŠæ—¶é—´ç‚¹çš„AUCç»“æœ
        if k == 0:  # åˆå§‹åŒ–æ—¶é—´ç‚¹AUCç»“æœå­—å…¸
            timepoint_auc_results = {timepoint: [] for timepoint in timepoint_aucs_test.keys()}
        for timepoint, auc_value in timepoint_aucs_test.items():
            timepoint_auc_results[timepoint].append(auc_value)
        
        logger.info(f"  - Fold {k+1} external test C-Index: {cindex_test:.4f}, I-AUC: {iauc_test:.4f}, I-Brier: {ibrier_test:.4f}")
        logger.info(f"    Time-dependent AUCs: 1-year: {timepoint_aucs_test['1-year']:.4f}, 2-year: {timepoint_aucs_test['2-year']:.4f}, 3-year: {timepoint_aucs_test['3-year']:.4f}, 5-year: {timepoint_aucs_test['5-year']:.4f}")

        per_fold_rows.append({
            'fold': k + 1,
            'test_cindex': float(cindex_test),
            'test_iauc': float(iauc_test),
            'test_ibrier': float(ibrier_test),
            'auc_1-year': float(timepoint_aucs_test.get('1-year', float('nan'))),
            'auc_2-year': float(timepoint_aucs_test.get('2-year', float('nan'))),
            'auc_3-year': float(timepoint_aucs_test.get('3-year', float('nan'))),
            'auc_5-year': float(timepoint_aucs_test.get('5-year', float('nan'))),
        })

    if not cindex_results:
        logger.warning("Failed to evaluate any folds on the external test set.")
        return float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), {}, {}
    
    # === ä¼ ç»Ÿæ¨¡å¼ï¼šè®¡ç®—æ¯ä¸ªæŠ˜ç»“æœçš„å‡å€¼å’Œæ ‡å‡†å·® ===
    mean_cindex = np.mean(cindex_results)
    std_cindex = np.std(cindex_results)
    mean_iauc = np.mean(iauc_results)
    std_iauc = np.std(iauc_results)
    mean_ibrier = np.mean(ibrier_results)
    std_ibrier = np.std(ibrier_results)
    
    # è®¡ç®—å››ä¸ªä¸´åºŠæ—¶é—´ç‚¹AUCçš„å‡å€¼å’Œæ ‡å‡†å·®
    timepoint_auc_stats = {}
    for timepoint in timepoint_auc_results.keys():
        timepoint_auc_stats[timepoint] = {
            'mean': np.mean(timepoint_auc_results[timepoint]),
            'std': np.std(timepoint_auc_results[timepoint])
        }
    
    # === èšåˆæ¨¡å¼ï¼šåŸºäºå¹³å‡é¢„æµ‹è®¡ç®—å•ä¸€æŒ‡æ ‡å€¼ ===
    from utils import CIndex_lifeline, integrated_brier_score, integrated_auc, clinical_timepoints_auc
    
    # è®¡ç®—å¹³å‡é¢„æµ‹åˆ†æ•°ï¼ˆäº”æŠ˜çš„å¹³å‡ï¼‰
    avg_predictions = np.mean(all_fold_predictions, axis=0)
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæŠ˜çš„ç”Ÿå­˜æ—¶é—´å’Œåˆ å¤±çŠ¶æ€ï¼ˆæ‰€æœ‰æŠ˜åº”è¯¥ç›¸åŒï¼‰
    final_survtime = all_fold_survtimes[0]
    final_censor = all_fold_censors[0]
    
    # åŸºäºå¹³å‡é¢„æµ‹è®¡ç®—èšåˆæŒ‡æ ‡
    aggregated_cindex = CIndex_lifeline(avg_predictions, final_censor, final_survtime)
    aggregated_iauc = integrated_auc(avg_predictions, final_censor, final_survtime)
    aggregated_ibrier = integrated_brier_score(avg_predictions, final_censor, final_survtime)
    aggregated_timepoint_aucs = clinical_timepoints_auc(avg_predictions, final_censor, final_survtime)
    
    # è·å–æ‚£è€…IDä¿¡æ¯ï¼šä¼˜å…ˆä½¿ç”¨å¤–éƒ¨æµ‹è¯•æ•°æ®ä¸­çš„ ids å­—æ®µ
    if 'ids' in external_test_data_raw:
        patient_ids = external_test_data_raw['ids']
    else:
        patient_ids = [f'Patient_{i}' for i in range(len(avg_predictions))]
    
    # ä¿å­˜èšåˆé¢„æµ‹åˆ†æ•°ç”¨äºåç»­åˆ†æ
    aggregated_predictions_data = {
        'patient_ids': patient_ids,
        'predictions': avg_predictions,
        'survtime': final_survtime,
        'censor': final_censor,
        'model_name': opt_exp.model_name,
        'exp_name': opt_exp.exp_name
    }
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    predictions_save_dir = os.path.join(opt_exp.checkpoints_dir, opt_exp.exp_name, 'aggregated_predictions')
    os.makedirs(predictions_save_dir, exist_ok=True)
    
    # ä¿å­˜ä¸ºpickleæ ¼å¼ï¼ˆç”¨äºç¨‹åºåŠ è½½ï¼‰
    predictions_save_path_pkl = os.path.join(predictions_save_dir, f'{opt_exp.model_name}_aggregated_predictions.pkl')
    with open(predictions_save_path_pkl, 'wb') as f:
        pickle.dump(aggregated_predictions_data, f)
    
    # ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆç”¨æˆ·å‹å¥½æ ¼å¼ï¼‰
    predictions_save_path_csv = os.path.join(predictions_save_dir, f'{opt_exp.model_name}_aggregated_predictions.csv')
    predictions_df = pd.DataFrame({
        'case_id': patient_ids,
        'aggregated_prediction': avg_predictions,
        'survival_time': final_survtime,
        'censor_status': final_censor,
        'model_name': opt_exp.model_name,
        'experiment_name': opt_exp.exp_name
    })
    predictions_df.to_csv(predictions_save_path_csv, index=False, float_format='%.6f')
    
    logger.info(f"Aggregated predictions saved to:")
    logger.info(f"  - Pickle format: {predictions_save_path_pkl}")
    logger.info(f"  - CSV format: {predictions_save_path_csv}")
    
    # å¦å­˜æ¯æŠ˜å¤–éƒ¨æµ‹è¯•ç»“æœåˆ°æ¨¡å‹ç›®å½•
    try:
        import pandas as _pd
        model_dir = os.path.join(opt_exp.checkpoints_dir, opt_exp.exp_name, opt_exp.model_name)
        os.makedirs(model_dir, exist_ok=True)
        _pd.DataFrame(per_fold_rows).to_csv(os.path.join(model_dir, 'external_results.csv'), index=False, float_format='%.6f')
    except Exception as e:
        logger.warning(f"Failed to save per-fold external results CSV: {e}")

    # åˆ›å»ºèšåˆç»“æœå­—å…¸
    aggregated_results = {
        'cindex': aggregated_cindex,
        'iauc': aggregated_iauc,
        'ibrier': aggregated_ibrier,
        'timepoint_aucs': aggregated_timepoint_aucs,
        'predictions_path_pkl': predictions_save_path_pkl,  # pickleæ–‡ä»¶è·¯å¾„
        'predictions_path_csv': predictions_save_path_csv   # CSVæ–‡ä»¶è·¯å¾„
    }
    
    logger.info(f"--- Traditional mode (fold-wise average): C-Index {mean_cindex:.4f} Â± {std_cindex:.4f}, I-AUC {mean_iauc:.4f} Â± {std_iauc:.4f}, I-Brier {mean_ibrier:.4f} Â± {std_ibrier:.4f} ---")
    logger.info(f"--- Aggregated mode (ensemble prediction): C-Index {aggregated_cindex:.4f}, I-AUC {aggregated_iauc:.4f}, I-Brier {aggregated_ibrier:.4f} ---")
    logger.info(f"--- Traditional time-dependent AUCs:")
    for timepoint, stats in timepoint_auc_stats.items():
        logger.info(f"    {timepoint}: {stats['mean']:.4f} Â± {stats['std']:.4f}")
    logger.info(f"--- Aggregated time-dependent AUCs:")
    for timepoint, auc_value in aggregated_timepoint_aucs.items():
        logger.info(f"    {timepoint}: {auc_value:.4f}")
    logger.info("---")
    
    return mean_cindex, std_cindex, mean_iauc, std_iauc, mean_ibrier, std_ibrier, timepoint_auc_stats, aggregated_results


def load_external_test_data(logger):
    """åŠ è½½å¤–éƒ¨æµ‹è¯•é›†æ•°æ®ã€‚"""
    external_test_path = os.path.join(EXTERNAL_DATA_DIR, 'external_test_data.pkl')
    try:
        with open(external_test_path, 'rb') as f:
            data = pickle.load(f)['test']
        logger.info(f"Successfully loaded external test set: {external_test_path}")
        return data
    except Exception as e:
        logger.error(f"Failed to load external test data: {e}. Please run 1_prepare_all_datasets.py first.", exc_info=True)
        return None

# =======================================================================================
# ä¸»å‡½æ•°
# =======================================================================================
def main():
    # 1. å‚æ•°è§£æå’Œæ—¥å¿—è®¾ç½®
    base_parser = get_base_parser()
    base_parser.add_argument('--use_optimized', action='store_true', default=True, help="Use optimized hyperparameters (default: True)")
    base_parser.add_argument('--use_baseline', action='store_true', help="Use baseline hyperparameters instead of optimized")
    # ä½¿ç”¨å®éªŒåç§°ï¼ˆexp['name']ï¼‰ä½œä¸ºé»˜è®¤è¿è¡Œåˆ—è¡¨ï¼Œè¿™æ ·å¯ä»¥åŒºåˆ†åŒä¸€æ¨¡å‹çš„ä¸åŒå˜ä½“
    all_model_names = [exp['name'] for exp in BASELINE_EXPERIMENTS]
    base_parser.add_argument('--models_to_run', nargs='+', type=str, default=all_model_names, help="Specify models to run")
    base_parser.add_argument('--pretrain_exp_name', type=str, default='', help='Experiment name containing pretrained weights')
    args = base_parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†use_baselineï¼Œåˆ™ä¸ä½¿ç”¨ä¼˜åŒ–å‚æ•°
    if args.use_baseline:
        args.use_optimized = False
    
    EVALUATION_NAME = args.exp_name if args.exp_name != 'fusion_experiment' else ('optimized_evaluation' if args.use_optimized else 'baseline_evaluation')
    logger_manager = LoggerManager(experiment_name=EVALUATION_NAME)
    logger = logger_manager.get_logger(__name__)
    
    logger.info("="*80 + f"\nğŸš€ Starting Model Evaluation Workflow ({'Optimized Params' if args.use_optimized else 'Default Baseline Params'})")
    logger.info(f"Experiment Set Name: {EVALUATION_NAME}")
    logger.info(f"Models to run: {', '.join(args.models_to_run)}" + "\n" + "="*80)
    
    external_data_raw = load_external_test_data(logger)
    if external_data_raw is None: return

    # 2. é€‰æ‹©å‚æ•°æ¥æº (é»˜è®¤ä½¿ç”¨ä¼˜åŒ–å‚æ•°)
    if args.use_optimized:
        # ä½¿ç”¨ OPTIMIZED_HYPERPARAMSï¼Œé”®ä¸ºæ¨¡å‹åï¼ˆå«å˜ä½“åç¼€ï¼‰
        experiments_source = {model: {'name': model, 'params': params} for model, params in OPTIMIZED_HYPERPARAMS.items()}
    else:
        # ä½¿ç”¨ BASELINE_EXPERIMENTSï¼Œé”®ä¸ºå®éªŒå
        experiments_source = {exp['name']: exp for exp in BASELINE_EXPERIMENTS}

    all_final_results = []
    # 3. å¾ªç¯æ‰§è¡Œå®éªŒ
    for model_name_in in args.models_to_run:
        # ç›´æ¥ä½¿ç”¨æ¨¡å‹åæŸ¥æ‰¾é…ç½®
        if model_name_in not in experiments_source:
            logger.warning(f"Configuration for model '{model_name_in}' not found in source, skipping.")
            continue
        exp_config = experiments_source[model_name_in]
        logger.info(f"\n{'='*80}\nâ¡ï¸  Running Experiment: {model_name_in}\n{'='*80}")

        opt = base_parser.parse_args([]) # Create a fresh opt object
        opt.exp_name = EVALUATION_NAME
        # ç»§æ‰¿å‘½ä»¤è¡Œå‚æ•°
        for key, value in vars(args).items(): setattr(opt, key, value)
        # åº”ç”¨é…ç½®ä¸­çš„å‚æ•°
        for key, value in exp_config['params'].items(): setattr(opt, key, value)
        # å¦‚æœ mode æœªè®¾ç½®ï¼Œæ ¹æ®æ¨¡å‹åæ¨æ–­ï¼ˆå»æ‰ _avg, _weighted ç­‰åç¼€ï¼‰
        if not hasattr(opt, 'mode') or not opt.mode:
            base_mode = model_name_in.split('_')[0] if '_' in model_name_in else model_name_in
            # å¤„ç†ç‰¹æ®Šæƒ…å†µï¼šLateFusionNet_avg -> LateFusionNet
            if model_name_in.startswith('LateFusionNet'):
                opt.mode = 'LateFusionNet'
            else:
                opt.mode = base_mode
        # ä½¿ç”¨å®Œæ•´çš„æ¨¡å‹åï¼ˆå«å˜ä½“åç¼€ï¼‰ä½œä¸ºä¿å­˜ç›®å½•å
        opt.model_name = model_name_in
        opt = parse_gpuids(opt)

        cv_results = run_cv_experiment(opt=opt, parser=base_parser, trial=None)
        mean_ext_ci, std_ext_ci, mean_ext_iauc, std_ext_iauc, mean_ext_ibrier, std_ext_ibrier, timepoint_auc_stats, aggregated_results = evaluate_folds_on_external(opt, external_data_raw, logger)

        result_dict = {
            'Model Architecture': exp_config['name'], 
            'CV Train C-Index': cv_results.get('mean_train_cindex', float('nan')),
            'CV Val C-Index': cv_results.get('mean_val_cindex', float('nan')), 
            'CV Val Std': cv_results.get('std_val_cindex', float('nan')),
            'CV Train I-AUC': cv_results.get('mean_train_iauc', float('nan')),
            'CV Val I-AUC': cv_results.get('mean_val_iauc', float('nan')),
            'CV Train I-Brier': cv_results.get('mean_train_ibrier', float('nan')),
            'CV Val I-Brier': cv_results.get('mean_val_ibrier', float('nan')),
            'External Test C-Index': mean_ext_ci, 
            'External Test Std': std_ext_ci,
            'External Test I-AUC': mean_ext_iauc,
            'External Test I-AUC Std': std_ext_iauc,
            'External Test I-Brier': mean_ext_ibrier,
            'External Test I-Brier Std': std_ext_ibrier
        }
        for timepoint, stats in timepoint_auc_stats.items():
            result_dict[f'External Test {timepoint} AUC'] = stats['mean']
            result_dict[f'External Test {timepoint} AUC Std'] = stats['std']
        result_dict['External Test C-Index (Aggregated)'] = aggregated_results['cindex']
        result_dict['External Test I-AUC (Aggregated)'] = aggregated_results['iauc']
        result_dict['External Test I-Brier (Aggregated)'] = aggregated_results['ibrier']
        for timepoint, auc_value in aggregated_results['timepoint_aucs'].items():
            result_dict[f'External Test {timepoint} AUC (Aggregated)'] = auc_value
        all_final_results.append(result_dict)

    # 4. ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    if all_final_results:
        df = pd.DataFrame(all_final_results).sort_values(by='External Test C-Index', ascending=False).reset_index(drop=True)
        
        logger.info("\n\n" + "="*120)
        logger.info("--- Final Performance Summary: Cross-Validation vs. External Test Set ---")
        logger.info("="*120 + "\n")
        
        # å®šä¹‰è¦æ˜¾ç¤ºçš„åˆ—é¡ºåº
        display_cols = [
            'Model Architecture', 
            'CV Train C-Index', 
            'CV Val C-Index', 
            'CV Val Std', # ã€æ–°å¢ã€‘
            'CV Train I-AUC',  # ã€æ–°å¢I-AUCã€‘
            'CV Val I-AUC',    # ã€æ–°å¢I-AUCã€‘
            'CV Train I-Brier',  # ã€æ–°å¢I-Brierã€‘
            'CV Val I-Brier',    # ã€æ–°å¢I-Brierã€‘
            'External Test C-Index', 
            'External Test Std',
            'External Test C-Index (Aggregated)',  # ã€æ–°å¢èšåˆC-Indexã€‘
            'External Test I-AUC',      # ã€æ–°å¢å¤–éƒ¨æµ‹è¯•I-AUCã€‘
            'External Test I-AUC Std',   # ã€æ–°å¢å¤–éƒ¨æµ‹è¯•I-AUCæ ‡å‡†å·®ã€‘
            'External Test I-AUC (Aggregated)',    # ã€æ–°å¢èšåˆI-AUCã€‘
            'External Test I-Brier',      # ã€æ–°å¢å¤–éƒ¨æµ‹è¯•I-Brierã€‘
            'External Test I-Brier Std',   # ã€æ–°å¢å¤–éƒ¨æµ‹è¯•I-Brieræ ‡å‡†å·®ã€‘
            'External Test I-Brier (Aggregated)',  # ã€æ–°å¢èšåˆI-Brierã€‘
            'External Test 1-year AUC',    # ã€æ–°å¢1å¹´AUCã€‘
            'External Test 1-year AUC Std', # ã€æ–°å¢1å¹´AUCæ ‡å‡†å·®ã€‘
            'External Test 1-year AUC (Aggregated)', # ã€æ–°å¢èšåˆ1å¹´AUCã€‘
            'External Test 2-year AUC',    # ã€æ–°å¢2å¹´AUCã€‘
            'External Test 2-year AUC Std', # ã€æ–°å¢2å¹´AUCæ ‡å‡†å·®ã€‘
            'External Test 2-year AUC (Aggregated)', # ã€æ–°å¢èšåˆ2å¹´AUCã€‘
            'External Test 3-year AUC',    # ã€æ–°å¢3å¹´AUCã€‘
            'External Test 3-year AUC Std', # ã€æ–°å¢3å¹´AUCæ ‡å‡†å·®ã€‘
            'External Test 3-year AUC (Aggregated)', # ã€æ–°å¢èšåˆ3å¹´AUCã€‘
            'External Test 5-year AUC',    # ã€æ–°å¢5å¹´AUCã€‘
            'External Test 5-year AUC Std',  # ã€æ–°å¢5å¹´AUCæ ‡å‡†å·®ã€‘
            'External Test 5-year AUC (Aggregated)'  # ã€æ–°å¢èšåˆ5å¹´AUCã€‘
        ]
        
        # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„åˆ—éƒ½å­˜åœ¨ï¼Œä»¥é˜²ä¸‡ä¸€
        for col in display_cols:
            if col not in df.columns:
                df[col] = float('nan')
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        logger.info("\n" + df[display_cols].to_string(index=False, float_format="%.4f"))
        logger.info("\n" + "="*120)
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶
        report_path = os.path.join(RESULTS_DIR, f'{EVALUATION_NAME}_complete_report.csv')
        df.to_csv(report_path, index=False, float_format="%.4f")
        logger.info(f"\nComplete report saved to: {report_path}")

if __name__ == '__main__':
    main()